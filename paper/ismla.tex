% !TeX program = xelatex
\documentclass[a4paper]{article}
\usepackage[parfill]{parskip}

\usepackage[backend=biber,style=authoryear]{biblatex}
\addbibresource{ismla.bib}
\usepackage[american]{babel}
\usepackage{fontspec,xunicode}
\usepackage[Latin,Malayalam]{ucharclasses}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{gb4e}

%\setmainfont{CMU Serif Roman}
\setmonofont[Scale=0.9]{Consolas}
%\newfontfamily{\defaultfont}{CMU Serif Roman}
\newfontfamily{\rachana}[Script=Malayalam,Scale=1.1]{Rachana}
\setTransitionTo{Malayalam}{\rachana}
\setTransitionFrom{Malayalam}{\fontfamily{lmr}\selectfont}

\newcommand{\typ}[1]{\texttt{#1}}
\newcommand{\afx}[1]{\textsc{#1}}

\author{Thora Daneyko}
\title{ISMLA Project: Malayalam Glosser}

\begin{document}

\maketitle

\begin{abstract}
Miau
\end{abstract}

%1) Provide the context of your software (Which need does it address? Are there any comparable tools for your language(s)? What does your tool do better?)
%2) A short overview of the challenges you faced on the linguistic side (if you are dealing with a single language, this would e.g. include examples illustrating the relevant features of your language).
%3) A quick glance at the relevant literature for your project (e.g. if you build a learning tool training a certain aspect of a language, mention any papers you come across dealing with this issue).
%4) A description of the architecture and the features of your system, motivating the design and engineering decisions. (this is the main part)
%5) A paragraph stating clearly who contributed which parts to the project.
%6) References for all the sources you used, much as in a normal term paper.

\section{Introduction}

Bla bla bla test മലയാളി bla bla.

\section{About Malayalam}

\subsection{General information}

\subsection{NLP challenges}

\subsubsection{Tokenization}

അരിപ്പെട്ടി arippeṭṭi `rice box' (ari+peṭṭi), പാൽക്കുപ്പി pālkkuppi `milk bottle' (pāl+kuppi) \parencite[p.~397]{asherKumari}

\begin{exe}
\ex
മേഘം പോലെ കറുപ്പുനിറഞ്ഞോടുകൂടിയവർ ആണ്. \\
\textit{Mēghaṁ pōle kaṟuppuniṟaññōṭukūṭiyavar āṇ˘.}
\glll
മേഘം പോലെ കറുപ്പ് നിറഞ്ഞോട് കൂടി അവർ ആണ് . \\
mēghaṁ pōle kaṟupp˘ niṟañ-ñ-ōṭ˘ kūṭi avar āṇ˘ . \\
cloud like black be.full-\afx{pstpart}-\afx{soc} with they be . \\
\trans `They are black like clouds.' \parencite[p.~179]{ascaryacudamani}
\end{exe}


\begin{exe}
\ex
അതിന് നിനക്കെന്താ? \\
\textit{Atin˘ ninakkentā?}
\glll
അതിന് നിനക്ക് എന്ത് ആണ് ? \\
at-in˘ nin-akk˘ ent˘ āṇ˘ ? \\
that-\afx{dat} you-\afx{dat} what be ? \\
\trans `Why do you care?' \parencite[p.~165]{moag}
\end{exe}

%\begin{exe}
%\ex
%ഇവിടെത്തന്നെയാണ് iviṭettanneyāṇ˘ \parencite[p.~179]{ascaryacudamani}
%\glll
%ഇവിടെ തന്നെ ആണ് \\
%iviṭe tanne āṇ˘ \\
%here EMPH be \\
%\trans `is (indeed) here'
%\end{exe}
%
%
%\begin{exe}
%\ex
%എനിക്ക് കാപ്പിയേ വേണമെന്നുള്ളൂ. Enikk˘ kāppiyē vēṇamennuḷḷū.
%\glll
%എനിക്ക് കാപ്പി ഏ വേണം എന്ന് ഉള്ളൂ . \\
%en-ikk˘ kāppi ē vēṇaṁ enn˘ uḷḷ-ū . \\
%I-DAT coffee EMPH want.PRS QUOT be-FUT . \\
%\trans `I only want coffee.'
%\end{exe}
%
%
%\begin{exe}
%\ex
%ഞാൻ ചന്തയിലെല്ലാം നോക്കി. ñān cantayilellāṁ nōkki.
%\glll
%ഞാൻ ചന്തയിൽ എല്ലാം നോക്കി . \\
%ñān canta-yil ellāṁ nōkk-i . \\
%I.NOM market-LOC all look-PST . \\
%\trans `I looked everywhere on the market.'
%\end{exe}

\section{The Malayalam Glosser}

Bla bla

\subsection{Transliteration}

\subsubsection{Supported Scripts}

\subsubsection{Transliterators}

MalayalamTranscriptor

\subsection{Morphology Generation}

MorphGen

\subsection{Tokenization}

MalayalamGlosser

\subsection{Dictionary lookup}

MalayalamDictionary

\subsubsection{Efficiency considerations}

Considering that the dictionary may be very large and that the main function of the Glosser is to look words up in this dictionary, being able to load and query it very quickly is essential for the performance of the Glosser. Hence, I experimented with a few alternatives for storing the dictionary data and investigated their efficiency. The tests elaborated below are not very exact or well-designed and were only meant to quickly assess the usefulness of the considered methods.

\subsubsection*{\typ{HashMap} vs. \typ{ReverseTrie}}

The straightforward way to represent a dictionary as a Java object is a \typ{HashMap}. Apart from being readily available and easy to use, querying a \typ{HashMap} is fast. However, this also means that all entries are stored as their complete String representation, which may consume quite a lot of space. Considering that the inflected forms of the words share most of their characters, a trie representation seemed quite suitable and might be able to save space compared to a simple \typ{HashMap}. Since Malayalam is exclusively suffixing, I programmed a \typ{ReverseTrie} which reads and retrieves the strings from last to first character, in order to save as much space as possible. A useful side effect of this is that the tokenizer does not need to look up all suffixes of a compound word in the dictionary, but can simply do a suffix search of the \typ{ReverseTrie} to get the longest contained suffix.

In order to compare the performance of a \typ{HashMap} and \typ{ReverseTrie} based dictionary, I measured the memory used by the program before loading the dictionary data and after creating the \typ{HashMap} and Trie (calculated as \typ{Runtime.totalMemory() - Runtime.freeMemory()} after a \typ{System.gc()} call). Then I let the dictionary find the longest known suffix of the test String \textit{aviṭeyuḷḷataṟiññu} (\textit{aviṭe uḷḷat˘ aṟiññu} ``knew (he) was there'') 1,000,000 times and measured the time needed by a MashMap and \typ{ReverseTrie} based dictionary (calculated using \typ{System.currentmillis()}). Finally, I rewrote the tokenizer to also work with a \typ{ReverseTrie} and tested how long tokenization of a short conversation from \textcite{moag} took it with the two dictionary types.

Despite the many shared suffixes, the \typ{HashMap} was smaller than the \typ{ReverseTrie}, taking up 8,318,164.8 bytes on average during five test runs, while the Trie required 12,590,051.2 bytes. However, the memory used by the \typ{HashMap} varied greatly, ranging from only 5,160,456 to 9,801,392 bytes, while the Trie always consumed almost exactly the same amount of memory. This indicates that the measurements might have been verfälscht by background processes such as the garbage collection. However, the \typ{HashMap} still seems to be considerably smaller.

As expected, the \typ{ReverseTrie} outperformed the \typ{HashMap} on the looped suffix search of \textit{aviṭeyuḷḷataṟiññu}. The Map took an average of 999 milliseconds during five test runs, while the Trie only needed 312.4 ms. However, the performance of the Trie was very unstable, ranging from 140 to 518 ms between runs, while the \typ{HashMap} always needed between 908 and 1049 ms, which is still much slower than the slowest suffix search of the Trie.

On a real Malayalam text, where only few words are long compounds such as \textit{aviṭeyuḷḷataṟiññu}, both methods were equally fast. During 10 glossings of the Moag conversation, the Map based tokenization took 156.3 ms on average and the Trie based tokenization 161.7 ms. Both ran very stable.

All in all, the \typ{HashMap} seems to be the better choice, since it is smaller than the Trie and equally fast on normal Malayalam texts. The Trie is faster when tokenizing long compound words, which however are not frequent enough to justify preferring it over the \typ{HashMap}.

\subsubsection*{File storage vs. Serialization}

Loading the dictionary data into the underlying \typ{HashMap} (or \typ{ReverseTrie}) takes a considerable amount of time at launch. Hence, I considered serializing the Map or Trie object to be able to load it quicker. Since the Java serialization is known to rather slow, I used the FST Fast Serialization library for my tests. I first read the dictionary data from the text file and created the \typ{HashMap} and \typ{ReverseTrie} from it, measuring the time needed. Then I serialized the two objects and took the time required to deserialize them.

During five test runs, parsing the text file into an object took 278.2 ms on average for the \typ{HashMap} and 310.6 ms for the Trie. Deserializing the same objects required 563.4 ms on average for the \typ{HashMap} and 339.8 ms for the Trie. Loading the data from a text file is thus faster than deserializing a previously created object.

The file storing the serialized \typ{ReverseTrie} was twice as large as the file with the \typ{HashMap}. This confirms my assertions from the previous section that the Trie takes more space than the \typ{HashMap}.


\subsection{UI Design}

\section{Conclusion}

\nocite{*}
\printbibliography

\end{document}